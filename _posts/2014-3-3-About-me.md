---
layout: post
title: Training arbitrarily deep neural networks with progressive training
---

Deep neural networks solve many problems. Deeper neural networks solve even more problems. Though the latter architectures used to be very prone to overfitting and difficult to train, recent research has shown impressive improvements (think ResNet, FractalNet and others).

I've been thinking for a few months about these (relatively) new architectures. I do believe that the use of deeper networks allows for more expressivity and scalability to complex datasets. Interestingly, these "ultra-deep" networks - as I like to call them - even though they have large numbers of parameters, do not suffer too much from overfitting. One particularly interesting phenomenon I noted is particularly visible in 

I recently went through Google Brain's "old" GoogLeNet paper.  
